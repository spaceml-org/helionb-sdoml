{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7be8d2",
   "metadata": {},
   "source": [
    "# SDO Machine Learning (SDO ML) Dataset\n",
    "\n",
    "In this notebook we demonstrate the process for interacting with a small sample of the SDO ML dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888cc91",
   "metadata": {},
   "source": [
    "*Meng Jin, Mark Cheung, & Paul Wright*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b23838",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac9035",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132abb8",
   "metadata": {},
   "source": [
    "### SDO Overview\n",
    "\n",
    "Since its launch in 2010, NASA’s Solar Dynamics Observatory (SDO; ([Pesnell et al. 2012](https://ui.adsabs.harvard.edu/abs/2012SoPh..275....3P/abstract)) has continuously monitored Sun's activity, delivering a wealth of valuable scientific data for heliophysics researchers with the use of three instruments:\n",
    "\n",
    "1. The Atmospheric Imaging Assembly (AIA; [Lemen et al. 2012](https://ui.adsabs.harvard.edu/abs/2012SoPh..275...17L/abstract)), which captures 4096 x 4096 resolution images (with 0.6 arcsecond pixel size) of the full Sun in two ultraviolet (centered at 1600, and 1700 Å), seven extreme ultraviolet (EUV; centered at 94, 131, 171, 193, 211, 304, and 335 Å), and one visible (centered at 4500 Å) wavelength band.\n",
    "\n",
    "\n",
    "2. The Helioseismic and Magnetic Imager (HMI; [Schou et al. 2012](https://ui.adsabs.harvard.edu/abs/2012SoPh..275..229S/abstract)) captures visible wavelength filtergrams of the full Sun at 4096 x 4096 resolution (a pixel size of 0.5 arcsecond), which are then processed into a number of data products, including photospheric Dopplergrams, line-of-sight magnetograms, and vector magnetograms ([Hoeksema et al. 2014](https://ui.adsabs.harvard.edu/abs/2014SoPh..289.3483H/abstract)).\n",
    "\n",
    "\n",
    "3. The EUV Variability Experiment (EVE; [Woods et al. 2012](https://ui.adsabs.harvard.edu/abs/2012SoPh..275..115W/abstract)) monitors the solar EUV spectral irradiance from 1 to 1050 Å. This is done by utilizing multiple EUV Grating Spectrographs (MEGS) that disperse EUV light from the full disk of the Sun and its corona onto a 1024 x 2048 charge coupled device (CCD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306eea9",
   "metadata": {},
   "source": [
    "<img src=\"spacecraft_detailed.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a30e9",
   "metadata": {},
   "source": [
    "**Figure 1:** The Solar Dynamics Observatory (SDO) spacecraft, shown with the three main instruments (AIA, EVE, and HMI) highlighted. Image courtesy of NASA (https://sdo.gsfc.nasa.gov)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58841ea",
   "metadata": {},
   "source": [
    "### The SDO ML Dataset\n",
    "The SDO ML Dataset (covering 2010 - 2018) was originally published as [Galvez et al (2019)](https://ui.adsabs.harvard.edu/abs/2019ApJS..242....7G/abstract), and is hosted on the Stanford Digital Repository in Numpy's compressed array format (.npz). \n",
    "\n",
    "In this notebook, we present an update to the work outlined in Galvez et al (2019). Specifically, we will concentrate on the SDO/AIA data, as this has been updated to account for a change in calibration after 20??. In addtion to the change in calibration, this updated format includes\n",
    "\n",
    "1. FITS header/keyword information (such as observation time, and exposure time).\n",
    "\n",
    "2. processes for continually updating the data until the present day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61eb61a",
   "metadata": {},
   "source": [
    "### Who is the SDO ML Dataset for?\n",
    "\n",
    "The sheer volume of structured scientific data recorded by SDO (over 18 PB, and counting) is ideal for a range machine learning tasks (from time-series, to computer vision), as well as more traditional approaches.\n",
    "\n",
    "While the level 1 data are easily accessible, pre-processing these data for scientific analysis often requires  specialized heliophysics (and instrument-specific) knowledge. This may act as an unnecessary hurdle for non-heliophysics machine learning researchers who may wish to experiment with datasets from the physical sciences, but are unaware of domain-specific nuances (e.g., that images must be spatially and temporally adjusted).\n",
    "\n",
    "This notebook demonstrates the process for interacting with a subset of the curated SDO (AIA) dataset, that is mission-ready for machine-learning applications. **Our aim is to supply this standardized dataset for heliophysicists who wish to use machine learning in their own research, as well as machine-learning researchers who wish to develop models specialized for the physical sciences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17238c84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ae9cc",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85551c92",
   "metadata": {},
   "source": [
    "The notebook is set out as follows:\n",
    "\n",
    "1. Setting up the notebook\n",
    "2. Reading and loading the data <br>\n",
    "2a. Selecting images based on header information <br>\n",
    "    2b. Selecting a subset of the data based on index <br>\n",
    "    2c. Downsampling the data (resolution)<br>\n",
    "    2d. Downsampling the data (temporally)<br>\n",
    "3. Generating a SunPy map\n",
    "4. Animating AIA data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce5614",
   "metadata": {},
   "source": [
    "## 1. Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gcsfs\n",
    "import zarr\n",
    "import sunpy.map\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sunpy.visualization import axis_labels_from_ctype, wcsaxes_compat\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is currently fine for interacting with notebooks from within the project\n",
    "gcs = gcsfs.GCSFileSystem(access=\"read_only\")\n",
    "loc = \"fdl-sdoml-v2/sdomlv2_small.zarr/\"\n",
    "store = gcsfs.GCSMap(loc, gcs=gcs, check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2afa49",
   "metadata": {},
   "source": [
    "## 2. Reading and loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7cf13",
   "metadata": {},
   "source": [
    "The SDO ML dataset is stored in the Zarr format, a format for the storage of chunked, compressed, N-dimensional arrays with Numpy dtype. For an in-depth overview, see https://zarr.readthedocs.io/en/stable/tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43782255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we create a group with the store data located on GCP.\n",
    "root = zarr.group(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using `root.tree()`, we are able to display the hierarchy (of `loc`).\n",
    "print(root.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a9f92",
   "metadata": {},
   "source": [
    "As shown in the tree, the heirachy consists of groups, each shown with their respective shape, and data type. In this example, we will primarily look at the 171 Å channel from 2010. This consists of 6135 512x512 images, stored as float32, and can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20879a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = root[\"2010\"][\"171A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483e1dd",
   "metadata": {},
   "source": [
    "We could have alternatively accessed the 2010 data as:\n",
    "\n",
    "```\n",
    "loc = 'fdl-sdoml-v2/sdomlv2_small.zarr/2010'\n",
    "```\n",
    "\n",
    "which becomes increasingly useful in the full dataset (where the heirachy contains years 2010 - present)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d661c",
   "metadata": {},
   "source": [
    "**Loading with Dask**\n",
    "\n",
    "We can then load this data into an array using dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.from_array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a3cd88",
   "metadata": {},
   "source": [
    "As shown above, the data has the shape (6135, 512, 512), and is split into 52 chunks of (120, 512, 512), each of 125.83 MB; this is further visualised on the right. The data is now in a form to be manipulated like a Numpy array. \n",
    "\n",
    "\n",
    "Depending on the use-case, we may wish to extract a subset of this data in various ways. In the following sections we step through a number of potential operations that we may wish to make with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13a102",
   "metadata": {},
   "source": [
    "### 2a. Selecting images based on header information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64a076",
   "metadata": {},
   "source": [
    "We can extract the exposure (and observation) time from the data attributes (the header information), and downsample our data based upon that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa28705",
   "metadata": {},
   "outputs": [],
   "source": [
    "exptime = np.array(data.attrs[\"EXPTIME\"])\n",
    "t_obs = np.array(data.attrs[\"T_OBS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cb901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualise exposure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select indices where the exposure time is less than 2 seconds\n",
    "index = np.where(exptime < 2.0)\n",
    "selected_images = da.from_array(data)[index[0], :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8517",
   "metadata": {},
   "source": [
    "### 2b. Selecting images based on indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fa7cf",
   "metadata": {},
   "source": [
    "While the data is not currently ordered by observation time, we can simple index the array to extract a number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ebf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the data based on index\n",
    "selected_images = da.from_array(data)[100:200, :, :]\n",
    "selected_header = {keys: values[100:200] for keys, values in data.attrs.items()}\n",
    "\n",
    "# and even compute the mean of the selected images\n",
    "print(f\"mean of selected images: {da.mean(selected_images).compute():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8afb70",
   "metadata": {},
   "source": [
    "### 2c. Downsampling the data (resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a1706",
   "metadata": {},
   "source": [
    "Downsample the 512x512 images to 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4670c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample the data to 256 by sampling every other pixel\n",
    "sub_index = np.arange(0, 512, 2)\n",
    "\n",
    "sub_image = da.from_array(data)[0:600, sub_index, :]\n",
    "sub_image = sub_image[:, :, sub_index]\n",
    "# is this becuase dask doesn't support nd fancy indexing?\n",
    "\n",
    "print(f\"mean of selected images: {da.mean(sub_image).compute():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169093d",
   "metadata": {},
   "source": [
    "### 2d. Downsampling the data (temporally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5fcf2",
   "metadata": {},
   "source": [
    "We can use pandas to downstample the data within a time-range. Here, we choose 1 day of observations at 12 minute frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.DataFrame(t_obs, index=np.arange(np.shape(t_obs)[0]), columns=[\"Time\"])\n",
    "df_time[\"Time\"] = pd.to_datetime(df_time[\"Time\"])\n",
    "\n",
    "# select times at a frequency of 12 minutes\n",
    "selected_times = pd.date_range(\n",
    "    start=\"2010-08-28 00:00:00\", end=\"2010-08-28 23:59:59\", freq=\"12T\", tz=\"UTC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf062b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_index = []\n",
    "for i in selected_times:\n",
    "    selected_index.append(np.argmin(abs(df_time[\"Time\"] - i)))\n",
    "# may be a more efficient way of doing this\n",
    "time_index = list(filter(lambda x: x > 0, selected_index))\n",
    "da.from_array(data)[time_index, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72795b",
   "metadata": {},
   "source": [
    "## 3. Generating a SunPy Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabc6de",
   "metadata": {},
   "source": [
    "SunPy is an open-source Python library for Solar Physics data analysis and visualization. \n",
    "\n",
    "In this section we will demonstrate how SunPy’s [Map](https://docs.sunpy.org/en/stable/api/sunpy.map.Map.html#sunpy.map.Map) with the Zarr-formatted data. We demonstrate this for a single index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify an image index\n",
    "img_index = time_index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe506fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the respective image, and header required for sunpy.map.Map()\n",
    "selected_image = da.from_array(data)[img_index, :, :]\n",
    "selected_headr = {keys: values[img_index] for keys, values in data.attrs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c232c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_map = sunpy.map.Map((np.array(selected_image), selected_headr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(projection=my_map)\n",
    "my_map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad71165",
   "metadata": {},
   "source": [
    "As this is then a SunPy Map object, we can manipulate it as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acccfac7",
   "metadata": {},
   "source": [
    "For more information about SunPy, and Map, see the SunPy project: https://readthedocs.org/projects/sunpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24e338",
   "metadata": {},
   "source": [
    "## 4. Generating an AIA animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = []\n",
    "\n",
    "for i in time_index:\n",
    "    selected_image = da.from_array(data)[i, :, :]\n",
    "    selected_headr = {keys: values[i] for keys, values in data.attrs.items()}\n",
    "    maps.append(sunpy.map.Map((np.array(selected_image), selected_headr)))\n",
    "\n",
    "sq = sunpy.map.Map(maps, sequence=True, sortby=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36696883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_sequence(sequence):\n",
    "    \"\"\"\n",
    "    An animation plotting routine to animate each element in MapSequence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : `sunpy.map.mapsequence.MapSequence`\n",
    "        a set of sunpy maps to animate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `matplotlib.animation.FuncAnimation`\n",
    "        A FuncAnimation instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(7,7),\n",
    "    #                       subplot_kw=dict(projection=sequence[0].wcs)\n",
    "    #                      )\n",
    "\n",
    "    ax = wcsaxes_compat.gca_wcs(sequence[0].wcs)\n",
    "    fig = ax.get_figure()\n",
    "    fig.figsize = (7, 7)\n",
    "\n",
    "    plot_obj = sequence[0].plot()\n",
    "    ax.set_title(sequence[0].latex_name)\n",
    "\n",
    "    cbar = plt.colorbar()\n",
    "\n",
    "    def update_fig(i):\n",
    "        ax.set_title(sequence[i].latex_name)\n",
    "\n",
    "        # set the data to that of the image being plot\n",
    "        # Each image should be scaled as the first.\n",
    "        plot_obj.set_data(sequence[i].data)\n",
    "\n",
    "        # reset the WCS to that of the image being plot\n",
    "        plot_obj.axes.reset_wcs(sequence[i].wcs)\n",
    "\n",
    "        wcsaxes_compat.default_wcs_grid(ax)\n",
    "        ax.set_xlabel(\n",
    "            axis_labels_from_ctype(\n",
    "                sequence[i].coordinate_system[0], sequence[i].spatial_units[0]\n",
    "            )\n",
    "        )\n",
    "        ax.set_ylabel(\n",
    "            axis_labels_from_ctype(\n",
    "                sequence[i].coordinate_system[1], sequence[i].spatial_units[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # basic test:\n",
    "        # expect the wcs of the plot_obj to follow the sequence\n",
    "        # print(sequence[i].wcs == plot_obj.axes.wcs, sequence[0].wcs == sequence[i].wcs)\n",
    "        return (plot_obj,)\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_fig, init_func=None, frames=len(sequence), interval=60, repeat=True\n",
    "    )\n",
    "    plt.close(fig)\n",
    "\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d84513",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate_sequence(sq)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a7e49",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724a4f8",
   "metadata": {},
   "source": [
    "This project was conducted during the 2018 NASA Frontier Development Lab (FDL) program, a public/private partnership between NASA and SETI and industry partners including NVIDIA Corporation, Lockheed Martin, and IBM. The authors thank IBM (especially Naeem Altaf) for generously providing computing resources on the IBM Cloud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
