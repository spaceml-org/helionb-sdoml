{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwawmSO1but0"
   },
   "source": [
    "# DeepEM: A Deep Learning Approach for DEM Inversion\n",
    "In this notebook we demonstrate training (and testing) DeepEM, a deep-learning approach for DEM inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S73gxBv0NAO"
   },
   "source": [
    "by Paul Wright$^{1}$, Mark Cheung$^{1,2}$, Rajat Thomas$^{3}$, Richard Galvez$^{4}$, Alexandre Szenicer$^{5}$, Meng Jin$^{2,6}$, Andrés Muñoz-Jaramillo$^{7}$, and David Fouhey$^{8}$\n",
    "\n",
    "$^{1}$Stanford University;\n",
    "$^{2}$Lockheed Martin Solar and Astrophysics Laboratory;\n",
    "$^{3}$University of Amsterdam;\n",
    "$^{4}$5x5 Technologies Inc;\n",
    "$^{5}$University of Oxford;\n",
    "$^{6}$SETI Institute;\n",
    "$^{7}$SouthWest Research Institute;\n",
    "$^{8}$University of Michigan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skC6wGmjbut6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPc7NQqy0NAQ"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIwPfxuCbut6"
   },
   "source": [
    "The intensity observed through optically-thin <i>SDO</i>/AIA filters (94 Å, 131 Å, 171 Å, 193 Å, 211 Å, 335 Å) can be related to the temperature distribution of the solar corona (the differential emission measure; DEM) as\n",
    "\n",
    "\\begin{equation}\n",
    "g_{i} = \\int_{T} K_{i}(T) \\xi(T) dT \\, .\n",
    "\\end{equation}\n",
    "\n",
    "In this equation, $g_{i}$ is the DN s$^{-1}$ px$^{-1}$ value in the $i$th SDO/AIA channel. This intensity corresponds to the $K_{i}(T)$ temperature response function, and the DEM, $\\xi(T)$, is in units of cm$^{-5}$ K$^{-1}$. The matrix formulation of this integral equation can be represented in the form $\\vec{g} = {\\bf K}\\vec{\\xi}$, however this problem is an ill-posed inverse problem, and any attempt to directly recover $\\vec{\\xi}$ leads to significant noise amplification. \n",
    "\n",
    "There are numerous methods to tackle mathematical problems of this kind, and there are an increasing number of methods in the literature for recovering the differential emission measure from <i>SDO</i>/AIA observations, including methods based techniques such as Tikhonov Regularisation (<a href=\"https://doi.org/10.1051/0004-6361/201117576\">Hannah & Kontar 2012</a>), on the concept of sparsity (<a href=\"https://doi.org/10.1088/0004-637X/807/2/143\">Cheung <i>et al</i> 2015</a>).\n",
    "\n",
    "Here we present a deep learning approach for DEM Inversion. <i>For this notebook</i>, DeepEM is a trained on one set of <i>SDO</i>/AIA observations (six optically thin channels; 6 x N x N) and DEM solutions (in 18 temperature bins from log$_{10}$T = 5.5 - 7.2, 18 x N x N; Cheung <i>et al</i> 2015) at a resolution of 512 x 512 (N = 512) using a 1x1 2D Convolutional Neural Network with a single hidden layer.\n",
    "\n",
    "The DeepEM method presented here takes every DEM solution with no regards to the quality or existence of the solution. As will be demonstrated, when this method is trained with a single set images and DEM solutions, the DeepEM solutions have a similar fidelity to Basis Pursuit (with a significantly increased computation speed), and additionally, the DeepEM solutions find positive solutions at every pixel, and reduced noise in the DEM solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9rI_8fA0NAR"
   },
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRMPsG1S0NAR"
   },
   "source": [
    "The notebook is set out as follows:\n",
    "\n",
    "1. Setting up the notebook <br>\n",
    "2. Training\n",
    "    * Step 1: Obtain Data and Basis Pursuit Solutions for Training <br>\n",
    "    * Step 2: Define the Model <br>\n",
    "    * Step 3: Train the Model <br>\n",
    "3. Testing the Model <br>\n",
    "4. Synthesize <i>SDO</i>/AIA Observations\n",
    "5. Doing this on your data <br>\n",
    "6. Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrglVPjY0NAS"
   },
   "source": [
    "## 1. Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlyVFsN8but7"
   },
   "outputs": [],
   "source": [
    "# This notebook has been written in PyTorch\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGgO6Fuabvxp"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZRW7e_Id5JS"
   },
   "outputs": [],
   "source": [
    "bucket_name = \"fdl-deepem\"\n",
    "\n",
    "storage_client = storage.Client.create_anonymous_client()\n",
    "bucket = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2gle6K3but8"
   },
   "outputs": [],
   "source": [
    "# cudaize determines if a gpu is available for training and testing\n",
    "def cudaize(obj):\n",
    "    return obj.cuda() if torch.cuda.is_available() else obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHsfw-5MogK0",
    "outputId": "bc6252af-f5ec-4a95-989b-9d5758dc6434"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == False:\n",
    "    print(\n",
    "        f'CUDA is unavailable. If you are running this notebook on Colab, go to Runtime > Change runtime type, and set \"GPU\"'\n",
    "    )\n",
    "else:\n",
    "    print(\"CUDA is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OO0eBqhx0NAV"
   },
   "outputs": [],
   "source": [
    "### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EouJshBQbut9"
   },
   "outputs": [],
   "source": [
    "def em_scale(y):\n",
    "    return np.sqrt(y / 1e25)\n",
    "\n",
    "\n",
    "def em_unscale(y):\n",
    "    return 1e25 * (y * y)\n",
    "\n",
    "\n",
    "def img_scale(x):\n",
    "    x2 = x\n",
    "    bad = np.where(x2 <= 0.0)\n",
    "    x2[bad] = 0.0\n",
    "    return np.sqrt(x2)\n",
    "\n",
    "\n",
    "def img_unscale(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp1bfHTfbut9"
   },
   "source": [
    "## 2. Training\n",
    "### Step 1: Obtain Data and Basis Pursuit Solutions for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZZwkMOSbut-"
   },
   "source": [
    "We first load the <i>SDO</i>/AIA images and Basis Pursuit DEM maps.\n",
    "\n",
    "N.B. While this simplified version of DeepEM has been trained on DEM maps from Basis Pursuit (Cheung <i>et al</i> 2015), we actively encourage the readers to try their favourite method for DEM inversion! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_daJURB1hSe1"
   },
   "outputs": [],
   "source": [
    "def list_files(bucket_name):\n",
    "    \"\"\"List all files in GCP bucket.\"\"\"\n",
    "    files = bucket.list_blobs()\n",
    "    fileList = [file.name for file in files if \".\" in file.name]\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKpjJ3O0kV8F"
   },
   "outputs": [],
   "source": [
    "dir = \"fdl-deepem2\"\n",
    "os.mkdir(dir)\n",
    "files = list_files(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7l9qGmgkq6X",
    "outputId": "2eaf6998-1c27-4004-c6d1-9431a2f65c2a"
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    blob = bucket.blob(file)\n",
    "    blob.download_to_filename(os.path.join(dir, file))\n",
    "    print(f\"downloaded {file} to {dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5KLg5XTbut-"
   },
   "outputs": [],
   "source": [
    "aia_files = [\"AIA_DEM_2011-01-27\", \"AIA_DEM_2011-02-22\", \"AIA_DEM_2011-03-20\"]\n",
    "em_cube_files = aia_files\n",
    "status_files = aia_files\n",
    "for k, (afile, emfile) in enumerate(zip(aia_files, em_cube_files)):\n",
    "    afile_name = afile + \".aia.npy\"\n",
    "    emfile_name = emfile + \".emcube.npy\"\n",
    "    status_name = emfile + \".status.npy\"\n",
    "    if k == 0:\n",
    "        X = np.load(os.path.join(dir, afile_name))\n",
    "        y = np.load(os.path.join(dir, emfile_name))\n",
    "        status = np.load(os.path.join(dir, status_name))\n",
    "\n",
    "        X = np.zeros((len(aia_files), X.shape[0], X.shape[1], X.shape[2]))\n",
    "        y = np.zeros((len(em_cube_files), y.shape[0], y.shape[1], y.shape[2]))\n",
    "        status = np.zeros((len(status_files), status.shape[0], status.shape[1]))\n",
    "\n",
    "        nlgT = y.shape[0]\n",
    "        lgtaxis = np.arange(y.shape[1]) * 0.1 + 5.5\n",
    "\n",
    "    X[k] = np.load(os.path.join(dir, afile_name))\n",
    "    y[k] = np.load(os.path.join(dir, emfile_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-aDYnenbut_"
   },
   "source": [
    "### Step 2: Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op77K9Gxbut_"
   },
   "source": [
    "We first define the model as a 1x1 2D Convolutional Neural Network (CNN) with a kernel size of 1x1 and a single hidden layer. The model accepts a data cube of 6 x N x N (<i>SDO</i>/AIA data), and returns a data cube of 18 x N x N (DEM). When trained, this will transform the input (each pixel of the 6 <i>SDO</i>/AIA channels; 6 x 1 x 1) to the output (DEM at each pixel; 18 x 1 x 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXFZ0MfJbuuA"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(6, 300, kernel_size=1),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(300, 300, kernel_size=1),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(300, 18, kernel_size=1),\n",
    ")\n",
    "\n",
    "model = cudaize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRcpzHhobuuA"
   },
   "source": [
    "### Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Yi_wwXvbuuB"
   },
   "source": [
    "For training, we select one <i>SDO</i>/AIA data cube (6 x 512 x 512) and the corresponding Basis Pursuit DEM output (18 x 512 x 512). In the case presented here, we train the CNN on an image of the Sun obtained on the 27 Jan  2011; validate on an image of the Sun obtained one synodic rotation later (+26 days; 22-02-2011); and finally test on an image another 26 days later (20-03-2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1jHFDZkbuuB"
   },
   "outputs": [],
   "source": [
    "X = img_scale(X)\n",
    "y = em_scale(y)\n",
    "\n",
    "X_train = X[0:1]\n",
    "y_train = y[0:1]\n",
    "\n",
    "X_val = X[1:2]\n",
    "y_val = y[1:2]\n",
    "\n",
    "X_test = X[2:3]\n",
    "y_test = y[2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJwxpt_lbuuB"
   },
   "source": [
    "#### Plotting SDO/AIA Observations ${\\it vs.}$ Basis Pursuit DEM bins\n",
    "\n",
    "For the test data set, the <i>SDO</i>/AIA images for 171 Å, 211 Å, and 94 Å, and the corresponding DEM bins near the peak sensitivity in these relative isothermal channel (log$_{10}$T = 5.9, 6.3, 7.0) are shown in Figure 1. Figure 1 shows a set of <i>SDO</i>/AIA images (171 Å, 211 Å, and 94 Å [top, left to right]) with the corresponding DEM maps (bottom) for temperature bins there are near the peak sensitivity of the <i>SDO</i>/AIA channel. Furthermore, it is clear from the DEM maps that a number of pixels that are $zero$. These pixels are primarily located off-disk, but there are a number of pixels on-disk that show this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "yGK5fg2-buuB",
    "outputId": "4a236944-09c9-483e-c71b-54583c52de8b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(X_test[0, 2, :, :], vmin=0.01, vmax=30, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"${\\it SDO}$/AIA 171 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(X_test[0, 4, :, :], vmin=0.25, vmax=25, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[1].text(5, 490, \"${\\it SDO}$/AIA 211 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(X_test[0, 0, :, :], vmin=0.01, vmax=3, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[2].text(5, 490, \"${\\it SDO}$/AIA 94 $\\AA$\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(y_test[0, 4, :, :], vmin=0.01, vmax=3, cmap=\"viridis\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"Basis Pursuit DEM log$_{10}$T ~ 5.9\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(y_test[0, 8, :, :], vmin=0.25, vmax=10, cmap=\"viridis\", origin=\"lower\")\n",
    "ax[1].text(5, 490, \"Basis Pursuit DEM log$_{10}$T ~ 6.3\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(y_test[0, 15, :, :], vmin=0.01, vmax=3, cmap=\"viridis\", origin=\"lower\")\n",
    "ax[2].text(5, 490, \"Basis Pursuit DEM log$_{10}$T ~ 7.0\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w8C9KO-buuD"
   },
   "source": [
    "<b>Figure 1:</b> Left to Right: <i>SDO</i>/AIA images in 171 Å, 211 Å, and 94 Å (top, left to right), with the corresponding DEM bins (chosen at the peak sensitivity of each of the <i>SDO</i>/AIA channels) shown below. In the DEM bins (bottom) it is clear that there are some pixels that have solutions of DEM = $zero$, as explicitly seen as dark regions/clusters of pixels on and off disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGwruOYnbuuD"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qgqUR5AbuuD"
   },
   "source": [
    "To implement training and testing of our model, we first define a DEMdata class, and define functions for training and validation/test: train_model, and valtest_model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrtySSUxbuuE"
   },
   "source": [
    "N.B. It is not necessary to train the model, and if required, the trained model can be loaded to the cpu as follows:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(6, 300, kernel_size=1),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(300, 300, kernel_size=1),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(300, 18, kernel_size=1))\n",
    "\n",
    "dem_model_file = 'DeepEM_CNN_HelioML.pth'\n",
    "model.load_state_dict(torch.load(dem_model_file))\n",
    "\n",
    "model = cudaize(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sDVo4sGbuuE"
   },
   "outputs": [],
   "source": [
    "class DEMdata(nn.Module):\n",
    "    def __init__(self, xtrain, ytrain, xtest, ytest, xval, yval, split=\"train\"):\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.x = xtrain\n",
    "            self.y = ytrain\n",
    "        if split == \"val\":\n",
    "            self.x = xval\n",
    "            self.y = yval\n",
    "        if split == \"test\":\n",
    "            self.x = xtest\n",
    "            self.y = ytest\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.x[index]).type(\n",
    "            torch.FloatTensor\n",
    "        ), torch.from_numpy(self.y[index]).type(torch.FloatTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWbCNSzQbuuE"
   },
   "outputs": [],
   "source": [
    "def train_model(dem_loader, criterion, optimizer, epochs=500):\n",
    "    model.train()\n",
    "    train_loss_all_batches = []\n",
    "    train_loss_epoch = []\n",
    "    train_val = []\n",
    "    for k in range(epochs):\n",
    "        count_ = 0\n",
    "        avg_loss = 0\n",
    "        # =================== progress indicator ==============\n",
    "        if k % ((epochs + 1) // 4) == 0:\n",
    "            print(\"[{0}]: {1:.1f}% complete: \".format(k, k / epochs * 100))\n",
    "        # =====================================================\n",
    "        for img, dem in dem_loader:\n",
    "            count_ += 1\n",
    "            optimizer.zero_grad()\n",
    "            # =================== forward =====================\n",
    "            img = cudaize(img)\n",
    "            dem = cudaize(dem)\n",
    "\n",
    "            output = model(img)\n",
    "            loss = criterion(output, dem)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_all_batches.append(loss.item())\n",
    "            avg_loss += loss.item()\n",
    "        # =================== Validation ===================\n",
    "        dem_data_val = DEMdata(\n",
    "            X_train, y_train, X_test, y_test, X_val, y_val, split=\"val\"\n",
    "        )\n",
    "        dem_loader_val = DataLoader(dem_data_val, batch_size=1)\n",
    "        val_loss, dummy, dem_pred_val, dem_in_test_val = valtest_model(\n",
    "            dem_loader_val, criterion\n",
    "        )\n",
    "\n",
    "        train_loss_epoch.append(avg_loss / count_)\n",
    "        train_val.append(val_loss)\n",
    "\n",
    "        if k % 10 == 0:\n",
    "            print(\n",
    "                \"Epoch: \",\n",
    "                k,\n",
    "                \"trn_loss: \",\n",
    "                avg_loss / count_,\n",
    "                \"val_loss: \",\n",
    "                train_val[k],\n",
    "            )\n",
    "\n",
    "    torch.save(model.state_dict(), \"DeepEM_CNN_HelioML.pth\")\n",
    "    return train_loss_epoch, train_val\n",
    "\n",
    "\n",
    "def valtest_model(dem_loader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    count = 0\n",
    "    test_loss = []\n",
    "    for img, dem in dem_loader:\n",
    "        count += 1\n",
    "        # =================== forward =====================\n",
    "        img = cudaize(img)\n",
    "        dem = cudaize(dem)\n",
    "\n",
    "        output = model(img)\n",
    "        loss = criterion(output, dem)\n",
    "        test_loss.append(loss.item())\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    return val_loss / count, test_loss, output, dem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67V9m2CAbuuE"
   },
   "source": [
    "We chose the Adam optimiser with a learning rate of 1e-4, and weight_decay set to 1e-9. We use Mean Squared Error (MSE) between the Basis Pursuit DEM map and the DeepEM map as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMY8-4jubuuE"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-9)\n",
    "criterion = cudaize(nn.MSELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtF16ySqbuuF"
   },
   "source": [
    "Using the defined functions, `dem_data` will return the training data, and this will be loaded by the `DataLoader` with batch_size=1 (one 512 x 512 image per batch). For each epoch, `train_loss` and `valdn_loss` will be returned by `train_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-330J0LbuuF",
    "outputId": "834f9761-6b43-4866-98f8-73d65a473747"
   },
   "outputs": [],
   "source": [
    "dem_data = DEMdata(X_train, y_train, X_test, y_test, X_val, y_val, split=\"train\")\n",
    "dem_loader = DataLoader(dem_data, batch_size=1)\n",
    "\n",
    "t0 = time.time()  # Timing how long it takes to predict the DEMs\n",
    "train_loss, valdn_loss = train_model(dem_loader, criterion, optimizer, epochs=500)\n",
    "ttime = \"Training time = {0} seconds\".format(time.time() - t0)\n",
    "print(ttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaYI8bcwbuuF"
   },
   "source": [
    "#### Plotting: MSE Loss for Training and Validation \n",
    "\n",
    "In order to understand how well the model has trained we plot the training loss and validation loss as a function of Epoch in Figure 2. Figure 2 shows the MSE loss for training (blue) and validation (orange) as a function of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "wd95GuwcbuuG",
    "outputId": "c8d75f6a-61dc-4da8-fc5a-c7d7c0eb4a95"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss, color=\"blue\")\n",
    "plt.plot(np.arange(len(train_loss)), valdn_loss, color=\"orange\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oY9RGis-buuG"
   },
   "source": [
    "<b>Figure 2:</b> Training and Validation MSE loss (blue, orange) as a function of Epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKa8DroWbuuG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7MxUdfybuuH"
   },
   "source": [
    "## 3. Testing the Model\n",
    "\n",
    "Now that the model has been trained, testing the model is a computationally cheap proceedure. As before, we choose the data using DEMdata, and load with DataLoader. Using `valtest_model`, the DeepEM map is created, and the MSE loss calculated as during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k16rNYwEbuuH"
   },
   "outputs": [],
   "source": [
    "dem_data_test = DEMdata(X_train, y_train, X_test, y_test, X_val, y_val, split=\"test\")\n",
    "dem_loader = DataLoader(dem_data_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pr6QVuQdbuuH"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()  # Timing how long it takes to predict the DEMs\n",
    "dummy, test_loss, dem_pred, dem_in_test = valtest_model(dem_loader, criterion)\n",
    "performance = \"Number of DEM solutions per second = {0}\".format(\n",
    "    (y_test.shape[2] * y_test.shape[3]) / (time.time() - t0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGKrSBhwbuuH",
    "outputId": "2a18d0a2-2814-43fe-c30d-ce29533ffac3"
   },
   "outputs": [],
   "source": [
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPHJzFvVbuuH"
   },
   "source": [
    "#### Plotting: AIA, Basis Pursuit, DeepEM \n",
    "\n",
    "With the DeepEM map calculated, we can now compare the solutions obtained by Basis Pursuit and DeepEM. Figure 3 is similar to Figure 1 with an additional row corresponding to the solutions for DeepEM. Figure 3 shows <i>SDO</i>/AIA images in 171 Å, 211 Å, and 94 Å (left, top to bottom), with the corresponding DEM bins from Basis Pursuit (chosen at the peak sensitivity of each of the <i>SDO</i>/AIA channels) shown in the middle (top to bottom). The right-hand column row shows the DeepEM solutions that correspond to the same bins as the Basis Pursuit solutions. DeepEM provides solutions that are similar to Basis Pursuit, but importantly, provides DEM solutions for every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "Vmf6FTSXbuuI",
    "outputId": "09a37f21-e610-4654-bb02-612737b7c335"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(X_test[0, 2, :, :], vmin=0.01, vmax=30, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"${\\it SDO}$/AIA 171 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(\n",
    "    dem_in_test[0, 4, :, :].cpu().detach().numpy(),\n",
    "    vmin=0.01,\n",
    "    vmax=3,\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax[1].text(5, 490, \"Basis Pursuit DEM log$_{10}$T ~ 5.9\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(\n",
    "    dem_pred[0, 4, :, :].cpu().detach().numpy(),\n",
    "    vmin=0.01,\n",
    "    vmax=3,\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax[2].text(5, 490, \"DeepEM log$_{10}$T ~ 5.9\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(X_test[0, 4, :, :], vmin=0.25, vmax=25, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"${\\it SDO}$/AIA 211 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(\n",
    "    dem_in_test[0, 8, :, :].cpu().detach().numpy(),\n",
    "    vmin=0.25,\n",
    "    vmax=10,\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax[1].text(5, 490, \"Basis Pursuit DEM log$_{10}$T ~ 6.3\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(\n",
    "    dem_pred[0, 8, :, :].cpu().detach().numpy(),\n",
    "    vmin=0.25,\n",
    "    vmax=10,\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax[2].text(5, 490, \"DeepEM log$_{10}$T ~ 6.3\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(X_test[0, 0, :, :], vmin=0.01, vmax=3, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"${\\it SDO}$/AIA 94 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(\n",
    "    dem_in_test[0, 15, :, :].cpu().detach().numpy(),\n",
    "    vmin=0.01,\n",
    "    vmax=3,\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax[1].text(5, 490, \"Basis Pursuit DEM log$_{10}$T ~ 7.0\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(\n",
    "    dem_pred[0, 15, :, :].cpu().detach().numpy(),\n",
    "    vmin=0.01,\n",
    "    vmax=3,\n",
    "    cmap=\"viridis\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax[2].text(5, 490, \"DeepEM log$_{10}$T ~ 7.0\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFmVVcmbuuI"
   },
   "source": [
    "<b>Figure 3</b>: Left to Right: <i>SDO</i>/AIA images in 171 Å, 211 Å, and 94 Å (left, top to bottom), with the corresponding DEM bins from Basis Pursuit (chosen at the peak sensitivity of each of the <i>SDO</i>/AIA channels) shown below (middle, top to bottom). The right-hand column shows the DeepEM solutions that correspond to the same bins as the Basis Pursuit solutions. DeepEM provides solutions that are similar to Basis Pursuit, but importantly, provides DEM solutions for every pixel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDgxezdjbuuI"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOzZLAGibuuI"
   },
   "source": [
    "Furthermore, as we have the original Basis Pursuit DEM solutions (\"the ground truth\"), we can compare the average DEM from Basis Pursuit to the average DEM from DeepEM, as they should be similar. Figure 4 shows the average Basis Pursuit DEM (black curve) and the DeepEM solution (dashed line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0j0i_ntbuuJ"
   },
   "outputs": [],
   "source": [
    "def PlotTotalEM(em_unscaled, em_pred_unscaled, lgtaxis, status):\n",
    "    mask = np.zeros([status.shape[0], status.shape[1]])\n",
    "    mask[np.where(status == 0.0)] = 1.0\n",
    "    nmask = np.sum(mask)\n",
    "\n",
    "    EM_tru_sum = np.zeros([lgtaxis.size])\n",
    "    EM_inv_sum = np.zeros([lgtaxis.size])\n",
    "\n",
    "    for i in range(lgtaxis.size):\n",
    "        EM_tru_sum[i] = np.sum(em_unscaled[0, i, :, :] * mask) / nmask\n",
    "        EM_inv_sum[i] = np.sum(em_pred_unscaled[0, i, :, :] * mask) / nmask\n",
    "\n",
    "    fig = plt.figure\n",
    "    plt.plot(lgtaxis, EM_tru_sum, linewidth=3, color=\"black\")\n",
    "    plt.plot(lgtaxis, EM_inv_sum, linewidth=3, color=\"lightblue\", linestyle=\"--\")\n",
    "    plt.tick_params(axis=\"both\", which=\"major\")  # , labelsize=16)\n",
    "    plt.tick_params(axis=\"both\", which=\"minor\")  # , labelsize=16)\n",
    "\n",
    "    dlogT = lgtaxis[1] - lgtaxis[0]\n",
    "\n",
    "    plt.xlim(lgtaxis[0] - 0.5 * dlogT, lgtaxis.max() + 0.5 * dlogT)\n",
    "    plt.xticks(np.arange(np.min(lgtaxis), np.max(lgtaxis), 2 * dlogT))\n",
    "    plt.ylim(1e24, 1e27)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"log$_{10}$T [K]\")\n",
    "    plt.ylabel(\"Mean Emission Measure [cm$^{-5}$]\")\n",
    "\n",
    "    plt.show()\n",
    "    return EM_inv_sum, EM_tru_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "MnQ1PtOLbuuJ",
    "outputId": "320d847c-5ecb-4199-e46f-870a3f98a362",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "em_unscaled = em_unscale(dem_in_test.detach().cpu().numpy())\n",
    "em_pred_unscaled = em_unscale(dem_pred.detach().cpu().numpy())\n",
    "\n",
    "# Status for the test data. While DeepEM was trained on all examples in the training set,\n",
    "# we only compare the DEMs where Basis Pursuit obtained a solution (status = 0)\n",
    "status = status[2, :, :]\n",
    "\n",
    "EMinv, EMTru = PlotTotalEM(em_unscaled, em_pred_unscaled, lgtaxis, status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKuxEuggbuuJ"
   },
   "source": [
    "<b>Figure 4</b>: Average Basis Pursuit DEM (plotted as mean emission measure, black line) and the Average DeepEM solution (dashed line). It is clear that this simple implementation of DeepEM provides, on average, DEMs that are similar to Basis Pursuit (Cheung <i>et al</i> 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f63TKyefbuuJ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE-G-cmqbuuK"
   },
   "source": [
    "## 4. Synthesize <i>SDO</i>/AIA Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NEO6mxYbuuK"
   },
   "source": [
    "Finally, it is also of interest to reconstruct the <i>SDO</i>/AIA observations from both the Basis Pursuit, and DeepEM solutions. \n",
    "\n",
    "We are able to pose the problem of reconstructing the <i>SDO</i>/AIA observations from the DEM as a 1x1 2D Convolution. We first define the weights as the response functions of each channel, and set the biases to $zero$. By convolving the unscaled DEM at each pixel with the 6 filters (one for each <i>SDO</i>/AIA response function), we can recover the <i>SDO</i>/AIA observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZYUCZA5buuK"
   },
   "outputs": [],
   "source": [
    "# We first load the AIA response functions:\n",
    "cl = np.load(os.path.join(dir, \"AIA_Resp.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBwjnymrbuuK"
   },
   "outputs": [],
   "source": [
    "# Used Conv2d to convolve?? every pixel (18x1x1) by the 6 response functions\n",
    "# to return a set of observed fluxes in each channel (6x1x1)\n",
    "dem2aia = cudaize(nn.Conv2d(18, 6, kernel_size=1))\n",
    "\n",
    "chianti_lines_2 = cudaize(torch.zeros(6, 18, 1, 1))\n",
    "biases = cudaize(torch.zeros(6))\n",
    "\n",
    "# set the weights to each of the SDO/AIA response functions and biases to zero\n",
    "for i, p in enumerate(dem2aia.parameters()):\n",
    "    if i == 0:\n",
    "        p.data = Variable(cudaize(torch.from_numpy(cl).type(torch.FloatTensor)))\n",
    "    else:\n",
    "        p.data = biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOeUuDihbuuK"
   },
   "outputs": [],
   "source": [
    "AIA_out = img_scale(dem2aia(Variable(em_unscale(dem_in_test))).detach().cpu().numpy())\n",
    "AIA_out_DeepEM = img_scale(\n",
    "    dem2aia(Variable(em_unscale(dem_pred))).detach().cpu().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4O1VXmZbuuL"
   },
   "source": [
    "#### Plotting SDO/AIA Observations and Synthetic Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "0KHYdzh7buuL",
    "outputId": "aafd2260-3ffa-4be3-d3ee-3cfc09712b3b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(X_test[0, 2, :, :], vmin=0.01, vmax=30, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"${\\it SDO}$/AIA 171 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(AIA_out[0, 2, :, :], vmin=0.01, vmax=30, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[1].text(5, 490, \"Basis Pursuit Synthesized 171 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(\n",
    "    AIA_out_DeepEM[0, 2, :, :], vmin=0.01, vmax=30, cmap=\"Greys_r\", origin=\"lower\"\n",
    ")\n",
    "ax[2].text(5, 490, \"DeepEM Synthesized 171 $\\AA$\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(X_test[0, 4, :, :], vmin=0.25, vmax=25, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"${\\it SDO}$/AIA 211 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(AIA_out[0, 4, :, :], vmin=0.25, vmax=25, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[1].text(5, 490, \"Basis Pursuit Synthesized 211 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(\n",
    "    AIA_out_DeepEM[0, 4, :, :], vmin=0.25, vmax=25, cmap=\"Greys_r\", origin=\"lower\"\n",
    ")\n",
    "ax[2].text(5, 490, \"DeepEM Synthesized 211 $\\AA$\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(9 * 2, 9))\n",
    "\n",
    "ax[0].imshow(X_test[0, 0, :, :], vmin=0.01, vmax=3, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[0].text(5, 490, \"${\\it SDO}$/AIA 94 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[1].imshow(AIA_out[0, 0, :, :], vmin=0.01, vmax=3, cmap=\"Greys_r\", origin=\"lower\")\n",
    "ax[1].text(5, 490, \"Basis Pursuit Synthesized 94 $\\AA$\", color=\"white\", size=\"large\")\n",
    "ax[2].imshow(\n",
    "    AIA_out_DeepEM[0, 0, :, :], vmin=0.01, vmax=3, cmap=\"Greys_r\", origin=\"lower\"\n",
    ")\n",
    "ax[2].text(5, 490, \"DeepEM Synthesized 94 $\\AA$\", color=\"white\", size=\"large\")\n",
    "\n",
    "for axes in ax:\n",
    "    axes.get_xaxis().set_visible(False)\n",
    "    axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXa4uDxFbuuM"
   },
   "source": [
    "<b>Figure 5:</b> Top to Bottom: <i>SDO</i>/AIA images in 171 Å, 211 Å, and 94 Å (left, top to bottom) with the corresponding synthesised observations from Basis Pursuit (middle, top to bottom) and DeepEM (right, top to bottom). DeepEM provides synthetic observations that are similar to Basis Pursuit, with the addition of solutions where the basis pursuit solution was $zero$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc1nbJ-XbuuM"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNlF8NqubuuM"
   },
   "source": [
    "## 5. Doing this on your data\n",
    "\n",
    "There are two way in which you can use this notebook:\n",
    "\n",
    "1. Train your own model: Instead of Basis Pursuit solutions (as used here), you could use your favourite inversion technique to generate the training data and then feed that into the training.\n",
    "\n",
    "2. Directly use the pre-trained model we provide and perform inference on your AIA images.\n",
    "\n",
    "#### Formats:\n",
    "\n",
    "* Input: 6 x N x N (where 6 is the number of input AIA/SDO channels) as .npy file\n",
    "* Output: Nt x N x N (where Nt is the number of temperature bins, 18 in our case)\n",
    "\n",
    "where N x N is the size of the image (assumed to be square)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnQPR8sVbuuM"
   },
   "source": [
    "## 6. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvthFnAgbuuM"
   },
   "source": [
    "This notebook has provided a simple example of how a 1x1 2D Convolutional Neural Network can be used to improve computational cost for DEM inversion. Future development of DeepEM is on-going, and this notebook can be improved in a few ways:\n",
    "\n",
    "1. By using both the original, and synthesised data from the DEM, the ability of the DEM to recover the original or supplementary data (such as spectroscopic EUV data) can be used as an additional term in the loss function. \n",
    "\n",
    "2. This implementation of DeepEM has been trained on a <i>single</i> set of observations. While there are 512$^{2}$ DEMs in one set of observations, it would be advisable to train the model to further images of the Sun in various states of activity including times of solar flaring.\n",
    "\n",
    "3. For simplicity this implementation of DeepEM has been trained on every single pixel in the training set with no with no regards to the quality or existence of the solution. If trained for enough Epochs, DeepEM will start to remember which combinations of AIA values lead to DEMs equal to zero in the original training set. By utilising the status files included in this notebook it would be advisable to only train DeepEM on pixels where the solutions exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--TFIFkHbuuN"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DerLkjKD4PTm"
   },
   "source": [
    "This project was initiated during the 2018 NASA Frontier Development Lab (FDL) program, a partnership between NASA, SETI, NVIDIA Corporation, Lockheed Martin, and Kx. We gratefully thank our mentors for guidance and useful discussion, as well as the SETI Institute for their hospitality."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "temperature_maps-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
